{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1T3dJal0TKR-bmoRHZPrGX8u2GSIu9CeK","authorship_tag":"ABX9TyNbgs8Jq4i9dW37WfjznpsD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","###Question 2 -\n","Implement 5 different CNN architectures with a comparison table for CIFAR 10\n","dataset using the PyTorch library\n","Note -\n","1. The model parameters for each architecture should not be more than 10000 parameters\n","2. Code comments should be given for proper code understanding\n","\n"],"metadata":{"id":"S0COQcJuZFLf"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchsummary import summary"],"metadata":{"id":"PGKSD_hmZEjy","executionInfo":{"status":"ok","timestamp":1685366989802,"user_tz":-330,"elapsed":599,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# normalize the data by specifying a mean and standard deviation for each of the three channels.\n","# This will convert the data from [0,1] to [-1,1] # Normalization of data should help speed up conversion and\n","# reduce the chance of vanishing gradients with certain activation functions.\n","transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data/', train=True,download=True,transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset,batch_size=4,shuffle=True)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True,transform=transform)\n","testloader = torch.utils.data.DataLoader(testset,batch_size=4,shuffle=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNcYlp6H6RTS","executionInfo":{"status":"ok","timestamp":1685367010294,"user_tz":-330,"elapsed":18135,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}},"outputId":"4b4550b0-4af0-46fa-a31d-71923754c48a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:11<00:00, 15326502.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data/\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","### define CNN Architecture1\n","class Net1(nn.Module):\n","  # CNN architecture 1\n","\n","    def __init__(self):\n","        super(Net1, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 5, kernel_size=3, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(5, 5, kernel_size=3, padding=1)\n","        self.relu2 = nn.ReLU()\n","        self.fc1 = nn.Linear(5 * 8 * 8, 20)\n","        self.relu3 = nn.ReLU()\n","        self.fc2 = nn.Linear(20, 10)\n","    def forward(self, x):\n","        x = self.pool(self.relu1(self.conv1(x)))\n","        x = self.pool(self.relu2(self.conv2(x)))\n","        x = x.view(x.size(0), -1)\n","        x = self.relu3(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","\n","\n"],"metadata":{"id":"sY2Gh9rt7WY7","executionInfo":{"status":"ok","timestamp":1685367010294,"user_tz":-330,"elapsed":19,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["### define CNN Architecture2\n","class Net2(nn.Module):\n","  # CNN architecture 1\n","\n","    def __init__(self):\n","        super(Net2, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, kernel_size=3, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 5, kernel_size=3, padding=1)\n","        self.relu2 = nn.ReLU()\n","        self.fc1 = nn.Linear(5 * 8* 8, 20)\n","        self.relu3 = nn.ReLU()\n","        self.fc2 = nn.Linear(20, 10)\n","    def forward(self, x):\n","        x = self.pool(self.relu1(self.conv1(x)))\n","        x = self.pool(self.relu2(self.conv2(x)))\n","        x = x.view(x.size(0), -1)\n","        x = self.relu3(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n"],"metadata":{"id":"7t4zVR_PDjjC","executionInfo":{"status":"ok","timestamp":1685367010295,"user_tz":-330,"elapsed":19,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["### define CNN Architecture3\n","class Net3(nn.Module):\n","  def __init__(self):\n","        super(Net3, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, kernel_size=3, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6,4, kernel_size=3, padding=1)\n","        self.relu2 = nn.ReLU()\n","        self.fc1 = nn.Linear(4*6*6, 20)\n","        self.relu3 = nn.ReLU()\n","        self.fc2 = nn.Linear(20, 10)\n","  def forward(self, x):\n","        x = self.pool(self.relu1(self.conv1(x)))\n","        x = self.pool(self.relu2(self.conv2(x)))\n","        x = x.view(x.size(0), -1)\n","        x = self.relu3(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"LGcRr6OnDjyj","executionInfo":{"status":"ok","timestamp":1685366778041,"user_tz":-330,"elapsed":1281,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["### define CNN Architecture3\n","class Net4(nn.Module):\n","  def __init__(self):\n","        super(Net4, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 4, 3, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(4, 8, 3, padding=1)\n","        self.relu2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(8* 6 * 6, 64)\n","        self.relu3 = nn.ReLU()\n","        self.fc2 = nn.Linear(64, 10)\n","\n","  def forward(self, x):\n","        x = self.pool1(self.relu1(self.conv1(x)))\n","        x = self.pool2(self.relu2(self.conv2(x)))\n","        x = x.view(x.size(0), -1)\n","        x = self.relu3(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n"],"metadata":{"id":"L8dMqv67Dj6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" ### define CNN Architecture5\n","class Net5(nn.Module):\n","     def __init__(self):\n","        super(Net5, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 4, 5, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(4, 16, 3, padding=1)\n","        self.relu2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(16 * 8 * 8, 64)\n","        self.relu3 = nn.ReLU()\n","        self.fc2 = nn.Linear(32, 10)\n","     def forward(self, x):\n","        x = self.pool1(self.relu1(self.conv1(x)))\n","        x = self.pool2(self.relu2(self.conv2(x)))\n","        x = x.view(x.size(0), -1)\n","        x = self.relu3(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n"],"metadata":{"id":"3qsuBuARDkBy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","##### Initialize the networks"],"metadata":{"id":"5Kkk8Aih_3Qc"}},{"cell_type":"code","source":["net1=Net1()"],"metadata":{"id":"lEhhS4627Whx","executionInfo":{"status":"ok","timestamp":1685367018584,"user_tz":-330,"elapsed":1128,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["net2=Net2()"],"metadata":{"id":"Gb_2_-Js7WqS","executionInfo":{"status":"ok","timestamp":1685367019519,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["net3=Net3()"],"metadata":{"id":"G0iDSX40-DTE","executionInfo":{"status":"ok","timestamp":1685366782562,"user_tz":-330,"elapsed":657,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["net4=Net4()"],"metadata":{"id":"sBH1tD5Q-FBL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net5=Net5()"],"metadata":{"id":"De2rGN-u-G_7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Stochastic gradient descent (SGD) with momentum to help get us over local minima and saddle points in the loss function space.\n","#### Define the loss function and optimizer"],"metadata":{"id":"J89rbFkr_K5X"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer1 = optim.SGD(net1.parameters(), lr=0.001, momentum=0.9)\n","optimizer2 = optim.SGD(net2.parameters(), lr=0.001, momentum=0.9)\n","optimizer3 = optim.SGD(net3.parameters(), lr=0.001, momentum=0.9)\n","optimizer4 = optim.SGD(net4.parameters(), lr=0.001, momentum=0.9)\n","optimizer5 = optim.SGD(net5.parameters(), lr=0.001, momentum=0.9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"VCcHfgYu_E9s","executionInfo":{"status":"error","timestamp":1685360232249,"user_tz":-330,"elapsed":427,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}},"outputId":"f8124267-4151-40fc-e984-4fe35941912d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-21937a7fe76c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moptimizer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moptimizer3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'net2' is not defined"]}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer1 = optim.SGD(net1.parameters(), lr=0.001, momentum=0.9)"],"metadata":{"id":"3e7YDV7CTTLW","executionInfo":{"status":"ok","timestamp":1685367026286,"user_tz":-330,"elapsed":1240,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer2= optim.SGD(net2.parameters(), lr=0.001, momentum=0.9)"],"metadata":{"id":"3-tDiIp4e1tl","executionInfo":{"status":"ok","timestamp":1685367030376,"user_tz":-330,"elapsed":835,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer3= optim.SGD(net3.parameters(), lr=0.001, momentum=0.9)"],"metadata":{"id":"rPMMgOPrhe9-","executionInfo":{"status":"ok","timestamp":1685366787051,"user_tz":-330,"elapsed":499,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"cyCyd5po_J8S"}},{"cell_type":"code","source":["\n","def get_variable_name(var):\n","    variables = globals()\n","    \n","    for name, value in variables.items():\n","        if value is var:\n","            return name\n","\n","# Train the networks\n","def train(net, optimizer, trainloader):\n","    var = get_variable_name(net)\n","    print(f\"Training started for {str.upper(var)}\\n\")\n","    net.train()\n","    for epoch in range(10):\n","        running_loss = 0.0\n","        for i, data in enumerate(trainloader, 0):\n","            inputs, labels = data\n","            optimizer.zero_grad()\n","\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            if i % 200 == 199:\n","                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n","                running_loss = 0.0\n","    print('\\n')\n","    print('Finished Training.')"],"metadata":{"id":"aOM-KK29AXbD","executionInfo":{"status":"ok","timestamp":1685367033142,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["### Test the networks\n","def test(net, testloader):\n","    net.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n"],"metadata":{"id":"atpGyd0jAqKa","executionInfo":{"status":"ok","timestamp":1685367035815,"user_tz":-330,"elapsed":7,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["#Train and test the networks\n","summary(net1, (3, 32, 32))\n","train(net1, optimizer1, trainloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N93ppye8JnuG","executionInfo":{"status":"ok","timestamp":1685361372026,"user_tz":-330,"elapsed":516384,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}},"outputId":"13759ff9-58ac-43ee-d13a-f0d8b5e1eb1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 5, 32, 32]             140\n","              ReLU-2            [-1, 5, 32, 32]               0\n","         MaxPool2d-3            [-1, 5, 16, 16]               0\n","            Conv2d-4            [-1, 5, 16, 16]             230\n","              ReLU-5            [-1, 5, 16, 16]               0\n","         MaxPool2d-6              [-1, 5, 8, 8]               0\n","            Linear-7                   [-1, 20]           6,420\n","              ReLU-8                   [-1, 20]               0\n","            Linear-9                   [-1, 10]             210\n","================================================================\n","Total params: 7,000\n","Trainable params: 7,000\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 0.11\n","Params size (MB): 0.03\n","Estimated Total Size (MB): 0.15\n","----------------------------------------------------------------\n","Training started for NET1\n","\n","[1,   200] loss: 2.302\n","[1,   400] loss: 2.291\n","[1,   600] loss: 2.262\n","[1,   800] loss: 2.209\n","[1,  1000] loss: 2.173\n","[1,  1200] loss: 2.117\n","[1,  1400] loss: 2.070\n","[1,  1600] loss: 2.019\n","[1,  1800] loss: 1.971\n","[1,  2000] loss: 1.947\n","[1,  2200] loss: 1.907\n","[1,  2400] loss: 1.832\n","[1,  2600] loss: 1.825\n","[1,  2800] loss: 1.855\n","[1,  3000] loss: 1.748\n","[1,  3200] loss: 1.768\n","[1,  3400] loss: 1.768\n","[1,  3600] loss: 1.770\n","[1,  3800] loss: 1.779\n","[1,  4000] loss: 1.699\n","[1,  4200] loss: 1.695\n","[1,  4400] loss: 1.683\n","[1,  4600] loss: 1.714\n","[1,  4800] loss: 1.655\n","[1,  5000] loss: 1.565\n","[1,  5200] loss: 1.640\n","[1,  5400] loss: 1.617\n","[1,  5600] loss: 1.597\n","[1,  5800] loss: 1.596\n","[1,  6000] loss: 1.646\n","[1,  6200] loss: 1.536\n","[1,  6400] loss: 1.578\n","[1,  6600] loss: 1.618\n","[1,  6800] loss: 1.572\n","[1,  7000] loss: 1.520\n","[1,  7200] loss: 1.517\n","[1,  7400] loss: 1.558\n","[1,  7600] loss: 1.615\n","[1,  7800] loss: 1.469\n","[1,  8000] loss: 1.519\n","[1,  8200] loss: 1.531\n","[1,  8400] loss: 1.524\n","[1,  8600] loss: 1.548\n","[1,  8800] loss: 1.418\n","[1,  9000] loss: 1.492\n","[1,  9200] loss: 1.507\n","[1,  9400] loss: 1.425\n","[1,  9600] loss: 1.501\n","[1,  9800] loss: 1.413\n","[1, 10000] loss: 1.511\n","[1, 10200] loss: 1.485\n","[1, 10400] loss: 1.404\n","[1, 10600] loss: 1.455\n","[1, 10800] loss: 1.482\n","[1, 11000] loss: 1.459\n","[1, 11200] loss: 1.480\n","[1, 11400] loss: 1.495\n","[1, 11600] loss: 1.473\n","[1, 11800] loss: 1.458\n","[1, 12000] loss: 1.483\n","[1, 12200] loss: 1.420\n","[1, 12400] loss: 1.457\n","[2,   200] loss: 1.412\n","[2,   400] loss: 1.414\n","[2,   600] loss: 1.427\n","[2,   800] loss: 1.463\n","[2,  1000] loss: 1.362\n","[2,  1200] loss: 1.354\n","[2,  1400] loss: 1.381\n","[2,  1600] loss: 1.431\n","[2,  1800] loss: 1.447\n","[2,  2000] loss: 1.344\n","[2,  2200] loss: 1.456\n","[2,  2400] loss: 1.434\n","[2,  2600] loss: 1.435\n","[2,  2800] loss: 1.405\n","[2,  3000] loss: 1.381\n","[2,  3200] loss: 1.377\n","[2,  3400] loss: 1.396\n","[2,  3600] loss: 1.465\n","[2,  3800] loss: 1.389\n","[2,  4000] loss: 1.333\n","[2,  4200] loss: 1.378\n","[2,  4400] loss: 1.374\n","[2,  4600] loss: 1.347\n","[2,  4800] loss: 1.394\n","[2,  5000] loss: 1.354\n","[2,  5200] loss: 1.410\n","[2,  5400] loss: 1.436\n","[2,  5600] loss: 1.381\n","[2,  5800] loss: 1.425\n","[2,  6000] loss: 1.364\n","[2,  6200] loss: 1.415\n","[2,  6400] loss: 1.399\n","[2,  6600] loss: 1.394\n","[2,  6800] loss: 1.293\n","[2,  7000] loss: 1.374\n","[2,  7200] loss: 1.407\n","[2,  7400] loss: 1.327\n","[2,  7600] loss: 1.391\n","[2,  7800] loss: 1.475\n","[2,  8000] loss: 1.404\n","[2,  8200] loss: 1.451\n","[2,  8400] loss: 1.280\n","[2,  8600] loss: 1.323\n","[2,  8800] loss: 1.339\n","[2,  9000] loss: 1.370\n","[2,  9200] loss: 1.341\n","[2,  9400] loss: 1.420\n","[2,  9600] loss: 1.409\n","[2,  9800] loss: 1.331\n","[2, 10000] loss: 1.369\n","[2, 10200] loss: 1.323\n","[2, 10400] loss: 1.377\n","[2, 10600] loss: 1.320\n","[2, 10800] loss: 1.381\n","[2, 11000] loss: 1.353\n","[2, 11200] loss: 1.417\n","[2, 11400] loss: 1.321\n","[2, 11600] loss: 1.401\n","[2, 11800] loss: 1.343\n","[2, 12000] loss: 1.373\n","[2, 12200] loss: 1.424\n","[2, 12400] loss: 1.315\n","[3,   200] loss: 1.287\n","[3,   400] loss: 1.327\n","[3,   600] loss: 1.286\n","[3,   800] loss: 1.298\n","[3,  1000] loss: 1.266\n","[3,  1200] loss: 1.329\n","[3,  1400] loss: 1.341\n","[3,  1600] loss: 1.273\n","[3,  1800] loss: 1.331\n","[3,  2000] loss: 1.335\n","[3,  2200] loss: 1.345\n","[3,  2400] loss: 1.318\n","[3,  2600] loss: 1.357\n","[3,  2800] loss: 1.288\n","[3,  3000] loss: 1.310\n","[3,  3200] loss: 1.327\n","[3,  3400] loss: 1.380\n","[3,  3600] loss: 1.343\n","[3,  3800] loss: 1.379\n","[3,  4000] loss: 1.330\n","[3,  4200] loss: 1.212\n","[3,  4400] loss: 1.346\n","[3,  4600] loss: 1.331\n","[3,  4800] loss: 1.281\n","[3,  5000] loss: 1.329\n","[3,  5200] loss: 1.301\n","[3,  5400] loss: 1.262\n","[3,  5600] loss: 1.275\n","[3,  5800] loss: 1.390\n","[3,  6000] loss: 1.348\n","[3,  6200] loss: 1.333\n","[3,  6400] loss: 1.374\n","[3,  6600] loss: 1.322\n","[3,  6800] loss: 1.235\n","[3,  7000] loss: 1.313\n","[3,  7200] loss: 1.271\n","[3,  7400] loss: 1.297\n","[3,  7600] loss: 1.323\n","[3,  7800] loss: 1.326\n","[3,  8000] loss: 1.336\n","[3,  8200] loss: 1.303\n","[3,  8400] loss: 1.315\n","[3,  8600] loss: 1.351\n","[3,  8800] loss: 1.279\n","[3,  9000] loss: 1.299\n","[3,  9200] loss: 1.367\n","[3,  9400] loss: 1.350\n","[3,  9600] loss: 1.338\n","[3,  9800] loss: 1.390\n","[3, 10000] loss: 1.304\n","[3, 10200] loss: 1.298\n","[3, 10400] loss: 1.376\n","[3, 10600] loss: 1.295\n","[3, 10800] loss: 1.281\n","[3, 11000] loss: 1.334\n","[3, 11200] loss: 1.359\n","[3, 11400] loss: 1.365\n","[3, 11600] loss: 1.286\n","[3, 11800] loss: 1.279\n","[3, 12000] loss: 1.223\n","[3, 12200] loss: 1.268\n","[3, 12400] loss: 1.304\n","[4,   200] loss: 1.288\n","[4,   400] loss: 1.239\n","[4,   600] loss: 1.298\n","[4,   800] loss: 1.263\n","[4,  1000] loss: 1.305\n","[4,  1200] loss: 1.264\n","[4,  1400] loss: 1.246\n","[4,  1600] loss: 1.273\n","[4,  1800] loss: 1.308\n","[4,  2000] loss: 1.245\n","[4,  2200] loss: 1.272\n","[4,  2400] loss: 1.231\n","[4,  2600] loss: 1.322\n","[4,  2800] loss: 1.231\n","[4,  3000] loss: 1.293\n","[4,  3200] loss: 1.236\n","[4,  3400] loss: 1.294\n","[4,  3600] loss: 1.265\n","[4,  3800] loss: 1.346\n","[4,  4000] loss: 1.249\n","[4,  4200] loss: 1.223\n","[4,  4400] loss: 1.369\n","[4,  4600] loss: 1.320\n","[4,  4800] loss: 1.301\n","[4,  5000] loss: 1.252\n","[4,  5200] loss: 1.273\n","[4,  5400] loss: 1.357\n","[4,  5600] loss: 1.227\n","[4,  5800] loss: 1.276\n","[4,  6000] loss: 1.322\n","[4,  6200] loss: 1.308\n","[4,  6400] loss: 1.264\n","[4,  6600] loss: 1.240\n","[4,  6800] loss: 1.321\n","[4,  7000] loss: 1.246\n","[4,  7200] loss: 1.305\n","[4,  7400] loss: 1.263\n","[4,  7600] loss: 1.271\n","[4,  7800] loss: 1.274\n","[4,  8000] loss: 1.235\n","[4,  8200] loss: 1.257\n","[4,  8400] loss: 1.252\n","[4,  8600] loss: 1.302\n","[4,  8800] loss: 1.340\n","[4,  9000] loss: 1.267\n","[4,  9200] loss: 1.331\n","[4,  9400] loss: 1.310\n","[4,  9600] loss: 1.301\n","[4,  9800] loss: 1.317\n","[4, 10000] loss: 1.289\n","[4, 10200] loss: 1.233\n","[4, 10400] loss: 1.326\n","[4, 10600] loss: 1.310\n","[4, 10800] loss: 1.277\n","[4, 11000] loss: 1.258\n","[4, 11200] loss: 1.184\n","[4, 11400] loss: 1.359\n","[4, 11600] loss: 1.318\n","[4, 11800] loss: 1.294\n","[4, 12000] loss: 1.273\n","[4, 12200] loss: 1.346\n","[4, 12400] loss: 1.281\n","[5,   200] loss: 1.146\n","[5,   400] loss: 1.261\n","[5,   600] loss: 1.251\n","[5,   800] loss: 1.291\n","[5,  1000] loss: 1.302\n","[5,  1200] loss: 1.210\n","[5,  1400] loss: 1.229\n","[5,  1600] loss: 1.238\n","[5,  1800] loss: 1.290\n","[5,  2000] loss: 1.249\n","[5,  2200] loss: 1.190\n","[5,  2400] loss: 1.230\n","[5,  2600] loss: 1.225\n","[5,  2800] loss: 1.261\n","[5,  3000] loss: 1.276\n","[5,  3200] loss: 1.318\n","[5,  3400] loss: 1.291\n","[5,  3600] loss: 1.249\n","[5,  3800] loss: 1.301\n","[5,  4000] loss: 1.215\n","[5,  4200] loss: 1.269\n","[5,  4400] loss: 1.205\n","[5,  4600] loss: 1.243\n","[5,  4800] loss: 1.273\n","[5,  5000] loss: 1.248\n","[5,  5200] loss: 1.309\n","[5,  5400] loss: 1.242\n","[5,  5600] loss: 1.286\n","[5,  5800] loss: 1.252\n","[5,  6000] loss: 1.323\n","[5,  6200] loss: 1.194\n","[5,  6400] loss: 1.301\n","[5,  6600] loss: 1.257\n","[5,  6800] loss: 1.278\n","[5,  7000] loss: 1.241\n","[5,  7200] loss: 1.303\n","[5,  7400] loss: 1.203\n","[5,  7600] loss: 1.282\n","[5,  7800] loss: 1.260\n","[5,  8000] loss: 1.258\n","[5,  8200] loss: 1.348\n","[5,  8400] loss: 1.180\n","[5,  8600] loss: 1.319\n","[5,  8800] loss: 1.284\n","[5,  9000] loss: 1.267\n","[5,  9200] loss: 1.342\n","[5,  9400] loss: 1.353\n","[5,  9600] loss: 1.283\n","[5,  9800] loss: 1.268\n","[5, 10000] loss: 1.275\n","[5, 10200] loss: 1.240\n","[5, 10400] loss: 1.174\n","[5, 10600] loss: 1.231\n","[5, 10800] loss: 1.263\n","[5, 11000] loss: 1.281\n","[5, 11200] loss: 1.256\n","[5, 11400] loss: 1.272\n","[5, 11600] loss: 1.283\n","[5, 11800] loss: 1.286\n","[5, 12000] loss: 1.193\n","[5, 12200] loss: 1.282\n","[5, 12400] loss: 1.271\n","[6,   200] loss: 1.226\n","[6,   400] loss: 1.266\n","[6,   600] loss: 1.349\n","[6,   800] loss: 1.145\n","[6,  1000] loss: 1.300\n","[6,  1200] loss: 1.216\n","[6,  1400] loss: 1.259\n","[6,  1600] loss: 1.269\n","[6,  1800] loss: 1.251\n","[6,  2000] loss: 1.200\n","[6,  2200] loss: 1.144\n","[6,  2400] loss: 1.184\n","[6,  2600] loss: 1.259\n","[6,  2800] loss: 1.239\n","[6,  3000] loss: 1.263\n","[6,  3200] loss: 1.270\n","[6,  3400] loss: 1.197\n","[6,  3600] loss: 1.264\n","[6,  3800] loss: 1.219\n","[6,  4000] loss: 1.217\n","[6,  4200] loss: 1.304\n","[6,  4400] loss: 1.165\n","[6,  4600] loss: 1.198\n","[6,  4800] loss: 1.229\n","[6,  5000] loss: 1.254\n","[6,  5200] loss: 1.256\n","[6,  5400] loss: 1.207\n","[6,  5600] loss: 1.182\n","[6,  5800] loss: 1.214\n","[6,  6000] loss: 1.218\n","[6,  6200] loss: 1.253\n","[6,  6400] loss: 1.243\n","[6,  6600] loss: 1.240\n","[6,  6800] loss: 1.248\n","[6,  7000] loss: 1.261\n","[6,  7200] loss: 1.369\n","[6,  7400] loss: 1.240\n","[6,  7600] loss: 1.229\n","[6,  7800] loss: 1.282\n","[6,  8000] loss: 1.261\n","[6,  8200] loss: 1.241\n","[6,  8400] loss: 1.330\n","[6,  8600] loss: 1.231\n","[6,  8800] loss: 1.289\n","[6,  9000] loss: 1.267\n","[6,  9200] loss: 1.252\n","[6,  9400] loss: 1.223\n","[6,  9600] loss: 1.262\n","[6,  9800] loss: 1.189\n","[6, 10000] loss: 1.168\n","[6, 10200] loss: 1.163\n","[6, 10400] loss: 1.231\n","[6, 10600] loss: 1.272\n","[6, 10800] loss: 1.223\n","[6, 11000] loss: 1.277\n","[6, 11200] loss: 1.255\n","[6, 11400] loss: 1.218\n","[6, 11600] loss: 1.258\n","[6, 11800] loss: 1.270\n","[6, 12000] loss: 1.236\n","[6, 12200] loss: 1.224\n","[6, 12400] loss: 1.218\n","[7,   200] loss: 1.196\n","[7,   400] loss: 1.238\n","[7,   600] loss: 1.148\n","[7,   800] loss: 1.224\n","[7,  1000] loss: 1.188\n","[7,  1200] loss: 1.241\n","[7,  1400] loss: 1.176\n","[7,  1600] loss: 1.217\n","[7,  1800] loss: 1.193\n","[7,  2000] loss: 1.258\n","[7,  2200] loss: 1.241\n","[7,  2400] loss: 1.203\n","[7,  2600] loss: 1.282\n","[7,  2800] loss: 1.209\n","[7,  3000] loss: 1.235\n","[7,  3200] loss: 1.233\n","[7,  3400] loss: 1.258\n","[7,  3600] loss: 1.156\n","[7,  3800] loss: 1.105\n","[7,  4000] loss: 1.252\n","[7,  4200] loss: 1.296\n","[7,  4400] loss: 1.235\n","[7,  4600] loss: 1.251\n","[7,  4800] loss: 1.349\n","[7,  5000] loss: 1.210\n","[7,  5200] loss: 1.247\n","[7,  5400] loss: 1.184\n","[7,  5600] loss: 1.229\n","[7,  5800] loss: 1.240\n","[7,  6000] loss: 1.195\n","[7,  6200] loss: 1.227\n","[7,  6400] loss: 1.224\n","[7,  6600] loss: 1.265\n","[7,  6800] loss: 1.275\n","[7,  7000] loss: 1.237\n","[7,  7200] loss: 1.224\n","[7,  7400] loss: 1.291\n","[7,  7600] loss: 1.206\n","[7,  7800] loss: 1.217\n","[7,  8000] loss: 1.185\n","[7,  8200] loss: 1.245\n","[7,  8400] loss: 1.165\n","[7,  8600] loss: 1.205\n","[7,  8800] loss: 1.229\n","[7,  9000] loss: 1.195\n","[7,  9200] loss: 1.329\n","[7,  9400] loss: 1.184\n","[7,  9600] loss: 1.209\n","[7,  9800] loss: 1.220\n","[7, 10000] loss: 1.248\n","[7, 10200] loss: 1.269\n","[7, 10400] loss: 1.232\n","[7, 10600] loss: 1.152\n","[7, 10800] loss: 1.236\n","[7, 11000] loss: 1.261\n","[7, 11200] loss: 1.321\n","[7, 11400] loss: 1.283\n","[7, 11600] loss: 1.246\n","[7, 11800] loss: 1.263\n","[7, 12000] loss: 1.301\n","[7, 12200] loss: 1.164\n","[7, 12400] loss: 1.227\n","[8,   200] loss: 1.254\n","[8,   400] loss: 1.186\n","[8,   600] loss: 1.237\n","[8,   800] loss: 1.185\n","[8,  1000] loss: 1.131\n","[8,  1200] loss: 1.177\n","[8,  1400] loss: 1.229\n","[8,  1600] loss: 1.272\n","[8,  1800] loss: 1.227\n","[8,  2000] loss: 1.221\n","[8,  2200] loss: 1.219\n","[8,  2400] loss: 1.196\n","[8,  2600] loss: 1.190\n","[8,  2800] loss: 1.160\n","[8,  3000] loss: 1.156\n","[8,  3200] loss: 1.261\n","[8,  3400] loss: 1.186\n","[8,  3600] loss: 1.155\n","[8,  3800] loss: 1.261\n","[8,  4000] loss: 1.209\n","[8,  4200] loss: 1.276\n","[8,  4400] loss: 1.193\n","[8,  4600] loss: 1.184\n","[8,  4800] loss: 1.231\n","[8,  5000] loss: 1.205\n","[8,  5200] loss: 1.184\n","[8,  5400] loss: 1.229\n","[8,  5600] loss: 1.224\n","[8,  5800] loss: 1.244\n","[8,  6000] loss: 1.168\n","[8,  6200] loss: 1.216\n","[8,  6400] loss: 1.168\n","[8,  6600] loss: 1.260\n","[8,  6800] loss: 1.178\n","[8,  7000] loss: 1.231\n","[8,  7200] loss: 1.093\n","[8,  7400] loss: 1.280\n","[8,  7600] loss: 1.222\n","[8,  7800] loss: 1.162\n","[8,  8000] loss: 1.256\n","[8,  8200] loss: 1.142\n","[8,  8400] loss: 1.232\n","[8,  8600] loss: 1.213\n","[8,  8800] loss: 1.306\n","[8,  9000] loss: 1.314\n","[8,  9200] loss: 1.207\n","[8,  9400] loss: 1.225\n","[8,  9600] loss: 1.268\n","[8,  9800] loss: 1.237\n","[8, 10000] loss: 1.186\n","[8, 10200] loss: 1.144\n","[8, 10400] loss: 1.202\n","[8, 10600] loss: 1.224\n","[8, 10800] loss: 1.272\n","[8, 11000] loss: 1.244\n","[8, 11200] loss: 1.247\n","[8, 11400] loss: 1.217\n","[8, 11600] loss: 1.246\n","[8, 11800] loss: 1.227\n","[8, 12000] loss: 1.336\n","[8, 12200] loss: 1.194\n","[8, 12400] loss: 1.167\n","[9,   200] loss: 1.221\n","[9,   400] loss: 1.125\n","[9,   600] loss: 1.239\n","[9,   800] loss: 1.239\n","[9,  1000] loss: 1.149\n","[9,  1200] loss: 1.269\n","[9,  1400] loss: 1.254\n","[9,  1600] loss: 1.169\n","[9,  1800] loss: 1.181\n","[9,  2000] loss: 1.151\n","[9,  2200] loss: 1.255\n","[9,  2400] loss: 1.233\n","[9,  2600] loss: 1.226\n","[9,  2800] loss: 1.232\n","[9,  3000] loss: 1.103\n","[9,  3200] loss: 1.236\n","[9,  3400] loss: 1.205\n","[9,  3600] loss: 1.223\n","[9,  3800] loss: 1.099\n","[9,  4000] loss: 1.219\n","[9,  4200] loss: 1.186\n","[9,  4400] loss: 1.274\n","[9,  4600] loss: 1.189\n","[9,  4800] loss: 1.274\n","[9,  5000] loss: 1.210\n","[9,  5200] loss: 1.200\n","[9,  5400] loss: 1.164\n","[9,  5600] loss: 1.193\n","[9,  5800] loss: 1.186\n","[9,  6000] loss: 1.196\n","[9,  6200] loss: 1.186\n","[9,  6400] loss: 1.179\n","[9,  6600] loss: 1.227\n","[9,  6800] loss: 1.245\n","[9,  7000] loss: 1.169\n","[9,  7200] loss: 1.261\n","[9,  7400] loss: 1.167\n","[9,  7600] loss: 1.270\n","[9,  7800] loss: 1.172\n","[9,  8000] loss: 1.189\n","[9,  8200] loss: 1.184\n","[9,  8400] loss: 1.133\n","[9,  8600] loss: 1.198\n","[9,  8800] loss: 1.224\n","[9,  9000] loss: 1.240\n","[9,  9200] loss: 1.221\n","[9,  9400] loss: 1.168\n","[9,  9600] loss: 1.205\n","[9,  9800] loss: 1.193\n","[9, 10000] loss: 1.219\n","[9, 10200] loss: 1.160\n","[9, 10400] loss: 1.241\n","[9, 10600] loss: 1.249\n","[9, 10800] loss: 1.184\n","[9, 11000] loss: 1.296\n","[9, 11200] loss: 1.202\n","[9, 11400] loss: 1.237\n","[9, 11600] loss: 1.262\n","[9, 11800] loss: 1.178\n","[9, 12000] loss: 1.212\n","[9, 12200] loss: 1.261\n","[9, 12400] loss: 1.165\n","[10,   200] loss: 1.127\n","[10,   400] loss: 1.166\n","[10,   600] loss: 1.200\n","[10,   800] loss: 1.186\n","[10,  1000] loss: 1.138\n","[10,  1200] loss: 1.165\n","[10,  1400] loss: 1.209\n","[10,  1600] loss: 1.175\n","[10,  1800] loss: 1.232\n","[10,  2000] loss: 1.134\n","[10,  2200] loss: 1.194\n","[10,  2400] loss: 1.193\n","[10,  2600] loss: 1.116\n","[10,  2800] loss: 1.235\n","[10,  3000] loss: 1.187\n","[10,  3200] loss: 1.203\n","[10,  3400] loss: 1.215\n","[10,  3600] loss: 1.229\n","[10,  3800] loss: 1.144\n","[10,  4000] loss: 1.180\n","[10,  4200] loss: 1.222\n","[10,  4400] loss: 1.223\n","[10,  4600] loss: 1.203\n","[10,  4800] loss: 1.235\n","[10,  5000] loss: 1.241\n","[10,  5200] loss: 1.120\n","[10,  5400] loss: 1.197\n","[10,  5600] loss: 1.217\n","[10,  5800] loss: 1.126\n","[10,  6000] loss: 1.232\n","[10,  6200] loss: 1.246\n","[10,  6400] loss: 1.201\n","[10,  6600] loss: 1.158\n","[10,  6800] loss: 1.151\n","[10,  7000] loss: 1.154\n","[10,  7200] loss: 1.214\n","[10,  7400] loss: 1.185\n","[10,  7600] loss: 1.176\n","[10,  7800] loss: 1.221\n","[10,  8000] loss: 1.131\n","[10,  8200] loss: 1.132\n","[10,  8400] loss: 1.189\n","[10,  8600] loss: 1.182\n","[10,  8800] loss: 1.212\n","[10,  9000] loss: 1.225\n","[10,  9200] loss: 1.238\n","[10,  9400] loss: 1.186\n","[10,  9600] loss: 1.260\n","[10,  9800] loss: 1.248\n","[10, 10000] loss: 1.228\n","[10, 10200] loss: 1.252\n","[10, 10400] loss: 1.230\n","[10, 10600] loss: 1.221\n","[10, 10800] loss: 1.192\n","[10, 11000] loss: 1.253\n","[10, 11200] loss: 1.266\n","[10, 11400] loss: 1.212\n","[10, 11600] loss: 1.287\n","[10, 11800] loss: 1.217\n","[10, 12000] loss: 1.194\n","[10, 12200] loss: 1.170\n","[10, 12400] loss: 1.255\n","\n","\n","Finished Training.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"1S5KZnj4RtPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(net2, (3, 32, 32))\n","train(net2, optimizer2, trainloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZlQED0cGzZc","executionInfo":{"status":"ok","timestamp":1685364416326,"user_tz":-330,"elapsed":541646,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}},"outputId":"b9faa63f-7cf3-46b9-c9bd-8d478c2cb232"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 6, 32, 32]             168\n","              ReLU-2            [-1, 6, 32, 32]               0\n","         MaxPool2d-3            [-1, 6, 16, 16]               0\n","            Conv2d-4            [-1, 5, 16, 16]             275\n","              ReLU-5            [-1, 5, 16, 16]               0\n","         MaxPool2d-6              [-1, 5, 8, 8]               0\n","            Linear-7                   [-1, 20]           6,420\n","              ReLU-8                   [-1, 20]               0\n","            Linear-9                   [-1, 10]             210\n","================================================================\n","Total params: 7,073\n","Trainable params: 7,073\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 0.13\n","Params size (MB): 0.03\n","Estimated Total Size (MB): 0.17\n","----------------------------------------------------------------\n","Training started for NET2\n","\n","[1,   200] loss: 2.306\n","[1,   400] loss: 2.286\n","[1,   600] loss: 2.258\n","[1,   800] loss: 2.206\n","[1,  1000] loss: 2.104\n","[1,  1200] loss: 2.079\n","[1,  1400] loss: 2.018\n","[1,  1600] loss: 2.020\n","[1,  1800] loss: 1.927\n","[1,  2000] loss: 1.905\n","[1,  2200] loss: 1.871\n","[1,  2400] loss: 1.867\n","[1,  2600] loss: 1.849\n","[1,  2800] loss: 1.802\n","[1,  3000] loss: 1.767\n","[1,  3200] loss: 1.861\n","[1,  3400] loss: 1.792\n","[1,  3600] loss: 1.731\n","[1,  3800] loss: 1.783\n","[1,  4000] loss: 1.711\n","[1,  4200] loss: 1.725\n","[1,  4400] loss: 1.719\n","[1,  4600] loss: 1.729\n","[1,  4800] loss: 1.686\n","[1,  5000] loss: 1.705\n","[1,  5200] loss: 1.719\n","[1,  5400] loss: 1.699\n","[1,  5600] loss: 1.703\n","[1,  5800] loss: 1.697\n","[1,  6000] loss: 1.693\n","[1,  6200] loss: 1.663\n","[1,  6400] loss: 1.584\n","[1,  6600] loss: 1.585\n","[1,  6800] loss: 1.628\n","[1,  7000] loss: 1.617\n","[1,  7200] loss: 1.652\n","[1,  7400] loss: 1.634\n","[1,  7600] loss: 1.708\n","[1,  7800] loss: 1.653\n","[1,  8000] loss: 1.530\n","[1,  8200] loss: 1.550\n","[1,  8400] loss: 1.559\n","[1,  8600] loss: 1.641\n","[1,  8800] loss: 1.591\n","[1,  9000] loss: 1.615\n","[1,  9200] loss: 1.567\n","[1,  9400] loss: 1.607\n","[1,  9600] loss: 1.539\n","[1,  9800] loss: 1.538\n","[1, 10000] loss: 1.541\n","[1, 10200] loss: 1.526\n","[1, 10400] loss: 1.572\n","[1, 10600] loss: 1.560\n","[1, 10800] loss: 1.568\n","[1, 11000] loss: 1.517\n","[1, 11200] loss: 1.569\n","[1, 11400] loss: 1.530\n","[1, 11600] loss: 1.546\n","[1, 11800] loss: 1.514\n","[1, 12000] loss: 1.611\n","[1, 12200] loss: 1.476\n","[1, 12400] loss: 1.448\n","[2,   200] loss: 1.512\n","[2,   400] loss: 1.499\n","[2,   600] loss: 1.509\n","[2,   800] loss: 1.457\n","[2,  1000] loss: 1.445\n","[2,  1200] loss: 1.452\n","[2,  1400] loss: 1.466\n","[2,  1600] loss: 1.529\n","[2,  1800] loss: 1.469\n","[2,  2000] loss: 1.451\n","[2,  2200] loss: 1.484\n","[2,  2400] loss: 1.408\n","[2,  2600] loss: 1.450\n","[2,  2800] loss: 1.472\n","[2,  3000] loss: 1.464\n","[2,  3200] loss: 1.442\n","[2,  3400] loss: 1.474\n","[2,  3600] loss: 1.424\n","[2,  3800] loss: 1.474\n","[2,  4000] loss: 1.495\n","[2,  4200] loss: 1.379\n","[2,  4400] loss: 1.424\n","[2,  4600] loss: 1.402\n","[2,  4800] loss: 1.462\n","[2,  5000] loss: 1.461\n","[2,  5200] loss: 1.434\n","[2,  5400] loss: 1.399\n","[2,  5600] loss: 1.435\n","[2,  5800] loss: 1.397\n","[2,  6000] loss: 1.485\n","[2,  6200] loss: 1.496\n","[2,  6400] loss: 1.422\n","[2,  6600] loss: 1.493\n","[2,  6800] loss: 1.461\n","[2,  7000] loss: 1.441\n","[2,  7200] loss: 1.403\n","[2,  7400] loss: 1.391\n","[2,  7600] loss: 1.386\n","[2,  7800] loss: 1.462\n","[2,  8000] loss: 1.396\n","[2,  8200] loss: 1.385\n","[2,  8400] loss: 1.432\n","[2,  8600] loss: 1.442\n","[2,  8800] loss: 1.358\n","[2,  9000] loss: 1.438\n","[2,  9200] loss: 1.401\n","[2,  9400] loss: 1.366\n","[2,  9600] loss: 1.464\n","[2,  9800] loss: 1.372\n","[2, 10000] loss: 1.425\n","[2, 10200] loss: 1.426\n","[2, 10400] loss: 1.454\n","[2, 10600] loss: 1.422\n","[2, 10800] loss: 1.426\n","[2, 11000] loss: 1.385\n","[2, 11200] loss: 1.321\n","[2, 11400] loss: 1.388\n","[2, 11600] loss: 1.371\n","[2, 11800] loss: 1.412\n","[2, 12000] loss: 1.393\n","[2, 12200] loss: 1.404\n","[2, 12400] loss: 1.335\n","[3,   200] loss: 1.406\n","[3,   400] loss: 1.335\n","[3,   600] loss: 1.313\n","[3,   800] loss: 1.335\n","[3,  1000] loss: 1.301\n","[3,  1200] loss: 1.340\n","[3,  1400] loss: 1.332\n","[3,  1600] loss: 1.334\n","[3,  1800] loss: 1.411\n","[3,  2000] loss: 1.359\n","[3,  2200] loss: 1.305\n","[3,  2400] loss: 1.362\n","[3,  2600] loss: 1.333\n","[3,  2800] loss: 1.407\n","[3,  3000] loss: 1.347\n","[3,  3200] loss: 1.348\n","[3,  3400] loss: 1.309\n","[3,  3600] loss: 1.286\n","[3,  3800] loss: 1.286\n","[3,  4000] loss: 1.338\n","[3,  4200] loss: 1.336\n","[3,  4400] loss: 1.300\n","[3,  4600] loss: 1.365\n","[3,  4800] loss: 1.378\n","[3,  5000] loss: 1.349\n","[3,  5200] loss: 1.329\n","[3,  5400] loss: 1.319\n","[3,  5600] loss: 1.340\n","[3,  5800] loss: 1.375\n","[3,  6000] loss: 1.298\n","[3,  6200] loss: 1.352\n","[3,  6400] loss: 1.403\n","[3,  6600] loss: 1.384\n","[3,  6800] loss: 1.298\n","[3,  7000] loss: 1.331\n","[3,  7200] loss: 1.428\n","[3,  7400] loss: 1.320\n","[3,  7600] loss: 1.327\n","[3,  7800] loss: 1.355\n","[3,  8000] loss: 1.297\n","[3,  8200] loss: 1.427\n","[3,  8400] loss: 1.315\n","[3,  8600] loss: 1.370\n","[3,  8800] loss: 1.308\n","[3,  9000] loss: 1.310\n","[3,  9200] loss: 1.251\n","[3,  9400] loss: 1.330\n","[3,  9600] loss: 1.330\n","[3,  9800] loss: 1.305\n","[3, 10000] loss: 1.331\n","[3, 10200] loss: 1.314\n","[3, 10400] loss: 1.340\n","[3, 10600] loss: 1.361\n","[3, 10800] loss: 1.325\n","[3, 11000] loss: 1.270\n","[3, 11200] loss: 1.348\n","[3, 11400] loss: 1.315\n","[3, 11600] loss: 1.304\n","[3, 11800] loss: 1.279\n","[3, 12000] loss: 1.393\n","[3, 12200] loss: 1.283\n","[3, 12400] loss: 1.292\n","[4,   200] loss: 1.302\n","[4,   400] loss: 1.223\n","[4,   600] loss: 1.280\n","[4,   800] loss: 1.245\n","[4,  1000] loss: 1.260\n","[4,  1200] loss: 1.312\n","[4,  1400] loss: 1.272\n","[4,  1600] loss: 1.291\n","[4,  1800] loss: 1.325\n","[4,  2000] loss: 1.308\n","[4,  2200] loss: 1.331\n","[4,  2400] loss: 1.343\n","[4,  2600] loss: 1.276\n","[4,  2800] loss: 1.283\n","[4,  3000] loss: 1.298\n","[4,  3200] loss: 1.333\n","[4,  3400] loss: 1.271\n","[4,  3600] loss: 1.310\n","[4,  3800] loss: 1.270\n","[4,  4000] loss: 1.313\n","[4,  4200] loss: 1.241\n","[4,  4400] loss: 1.280\n","[4,  4600] loss: 1.334\n","[4,  4800] loss: 1.251\n","[4,  5000] loss: 1.298\n","[4,  5200] loss: 1.258\n","[4,  5400] loss: 1.246\n","[4,  5600] loss: 1.283\n","[4,  5800] loss: 1.315\n","[4,  6000] loss: 1.343\n","[4,  6200] loss: 1.225\n","[4,  6400] loss: 1.302\n","[4,  6600] loss: 1.360\n","[4,  6800] loss: 1.252\n","[4,  7000] loss: 1.248\n","[4,  7200] loss: 1.317\n","[4,  7400] loss: 1.349\n","[4,  7600] loss: 1.378\n","[4,  7800] loss: 1.293\n","[4,  8000] loss: 1.225\n","[4,  8200] loss: 1.237\n","[4,  8400] loss: 1.348\n","[4,  8600] loss: 1.252\n","[4,  8800] loss: 1.293\n","[4,  9000] loss: 1.287\n","[4,  9200] loss: 1.209\n","[4,  9400] loss: 1.262\n","[4,  9600] loss: 1.361\n","[4,  9800] loss: 1.293\n","[4, 10000] loss: 1.233\n","[4, 10200] loss: 1.295\n","[4, 10400] loss: 1.266\n","[4, 10600] loss: 1.319\n","[4, 10800] loss: 1.326\n","[4, 11000] loss: 1.279\n","[4, 11200] loss: 1.375\n","[4, 11400] loss: 1.217\n","[4, 11600] loss: 1.297\n","[4, 11800] loss: 1.392\n","[4, 12000] loss: 1.308\n","[4, 12200] loss: 1.360\n","[4, 12400] loss: 1.294\n","[5,   200] loss: 1.254\n","[5,   400] loss: 1.214\n","[5,   600] loss: 1.209\n","[5,   800] loss: 1.278\n","[5,  1000] loss: 1.255\n","[5,  1200] loss: 1.230\n","[5,  1400] loss: 1.256\n","[5,  1600] loss: 1.227\n","[5,  1800] loss: 1.283\n","[5,  2000] loss: 1.337\n","[5,  2200] loss: 1.221\n","[5,  2400] loss: 1.341\n","[5,  2600] loss: 1.282\n","[5,  2800] loss: 1.368\n","[5,  3000] loss: 1.285\n","[5,  3200] loss: 1.267\n","[5,  3400] loss: 1.249\n","[5,  3600] loss: 1.279\n","[5,  3800] loss: 1.225\n","[5,  4000] loss: 1.192\n","[5,  4200] loss: 1.237\n","[5,  4400] loss: 1.218\n","[5,  4600] loss: 1.266\n","[5,  4800] loss: 1.307\n","[5,  5000] loss: 1.288\n","[5,  5200] loss: 1.238\n","[5,  5400] loss: 1.250\n","[5,  5600] loss: 1.262\n","[5,  5800] loss: 1.236\n","[5,  6000] loss: 1.209\n","[5,  6200] loss: 1.301\n","[5,  6400] loss: 1.290\n","[5,  6600] loss: 1.246\n","[5,  6800] loss: 1.270\n","[5,  7000] loss: 1.322\n","[5,  7200] loss: 1.333\n","[5,  7400] loss: 1.254\n","[5,  7600] loss: 1.237\n","[5,  7800] loss: 1.286\n","[5,  8000] loss: 1.239\n","[5,  8200] loss: 1.268\n","[5,  8400] loss: 1.253\n","[5,  8600] loss: 1.191\n","[5,  8800] loss: 1.303\n","[5,  9000] loss: 1.236\n","[5,  9200] loss: 1.226\n","[5,  9400] loss: 1.308\n","[5,  9600] loss: 1.157\n","[5,  9800] loss: 1.234\n","[5, 10000] loss: 1.288\n","[5, 10200] loss: 1.239\n","[5, 10400] loss: 1.266\n","[5, 10600] loss: 1.226\n","[5, 10800] loss: 1.263\n","[5, 11000] loss: 1.194\n","[5, 11200] loss: 1.235\n","[5, 11400] loss: 1.318\n","[5, 11600] loss: 1.320\n","[5, 11800] loss: 1.238\n","[5, 12000] loss: 1.152\n","[5, 12200] loss: 1.227\n","[5, 12400] loss: 1.267\n","[6,   200] loss: 1.225\n","[6,   400] loss: 1.230\n","[6,   600] loss: 1.155\n","[6,   800] loss: 1.222\n","[6,  1000] loss: 1.225\n","[6,  1200] loss: 1.215\n","[6,  1400] loss: 1.204\n","[6,  1600] loss: 1.250\n","[6,  1800] loss: 1.261\n","[6,  2000] loss: 1.233\n","[6,  2200] loss: 1.261\n","[6,  2400] loss: 1.256\n","[6,  2600] loss: 1.161\n","[6,  2800] loss: 1.298\n","[6,  3000] loss: 1.350\n","[6,  3200] loss: 1.162\n","[6,  3400] loss: 1.259\n","[6,  3600] loss: 1.247\n","[6,  3800] loss: 1.262\n","[6,  4000] loss: 1.205\n","[6,  4200] loss: 1.251\n","[6,  4400] loss: 1.254\n","[6,  4600] loss: 1.246\n","[6,  4800] loss: 1.255\n","[6,  5000] loss: 1.275\n","[6,  5200] loss: 1.240\n","[6,  5400] loss: 1.222\n","[6,  5600] loss: 1.176\n","[6,  5800] loss: 1.216\n","[6,  6000] loss: 1.240\n","[6,  6200] loss: 1.262\n","[6,  6400] loss: 1.272\n","[6,  6600] loss: 1.269\n","[6,  6800] loss: 1.266\n","[6,  7000] loss: 1.206\n","[6,  7200] loss: 1.208\n","[6,  7400] loss: 1.275\n","[6,  7600] loss: 1.287\n","[6,  7800] loss: 1.231\n","[6,  8000] loss: 1.175\n","[6,  8200] loss: 1.244\n","[6,  8400] loss: 1.292\n","[6,  8600] loss: 1.164\n","[6,  8800] loss: 1.253\n","[6,  9000] loss: 1.229\n","[6,  9200] loss: 1.289\n","[6,  9400] loss: 1.210\n","[6,  9600] loss: 1.256\n","[6,  9800] loss: 1.216\n","[6, 10000] loss: 1.231\n","[6, 10200] loss: 1.227\n","[6, 10400] loss: 1.203\n","[6, 10600] loss: 1.193\n","[6, 10800] loss: 1.308\n","[6, 11000] loss: 1.294\n","[6, 11200] loss: 1.216\n","[6, 11400] loss: 1.251\n","[6, 11600] loss: 1.226\n","[6, 11800] loss: 1.235\n","[6, 12000] loss: 1.257\n","[6, 12200] loss: 1.217\n","[6, 12400] loss: 1.279\n","[7,   200] loss: 1.156\n","[7,   400] loss: 1.218\n","[7,   600] loss: 1.226\n","[7,   800] loss: 1.215\n","[7,  1000] loss: 1.171\n","[7,  1200] loss: 1.217\n","[7,  1400] loss: 1.279\n","[7,  1600] loss: 1.204\n","[7,  1800] loss: 1.207\n","[7,  2000] loss: 1.315\n","[7,  2200] loss: 1.225\n","[7,  2400] loss: 1.239\n","[7,  2600] loss: 1.209\n","[7,  2800] loss: 1.165\n","[7,  3000] loss: 1.297\n","[7,  3200] loss: 1.334\n","[7,  3400] loss: 1.198\n","[7,  3600] loss: 1.177\n","[7,  3800] loss: 1.176\n","[7,  4000] loss: 1.238\n","[7,  4200] loss: 1.228\n","[7,  4400] loss: 1.199\n","[7,  4600] loss: 1.235\n","[7,  4800] loss: 1.227\n","[7,  5000] loss: 1.257\n","[7,  5200] loss: 1.243\n","[7,  5400] loss: 1.229\n","[7,  5600] loss: 1.199\n","[7,  5800] loss: 1.233\n","[7,  6000] loss: 1.194\n","[7,  6200] loss: 1.163\n","[7,  6400] loss: 1.253\n","[7,  6600] loss: 1.217\n","[7,  6800] loss: 1.188\n","[7,  7000] loss: 1.254\n","[7,  7200] loss: 1.199\n","[7,  7400] loss: 1.292\n","[7,  7600] loss: 1.166\n","[7,  7800] loss: 1.269\n","[7,  8000] loss: 1.239\n","[7,  8200] loss: 1.195\n","[7,  8400] loss: 1.195\n","[7,  8600] loss: 1.233\n","[7,  8800] loss: 1.228\n","[7,  9000] loss: 1.262\n","[7,  9200] loss: 1.184\n","[7,  9400] loss: 1.198\n","[7,  9600] loss: 1.264\n","[7,  9800] loss: 1.210\n","[7, 10000] loss: 1.179\n","[7, 10200] loss: 1.190\n","[7, 10400] loss: 1.305\n","[7, 10600] loss: 1.256\n","[7, 10800] loss: 1.174\n","[7, 11000] loss: 1.189\n","[7, 11200] loss: 1.251\n","[7, 11400] loss: 1.245\n","[7, 11600] loss: 1.209\n","[7, 11800] loss: 1.244\n","[7, 12000] loss: 1.242\n","[7, 12200] loss: 1.287\n","[7, 12400] loss: 1.237\n","[8,   200] loss: 1.217\n","[8,   400] loss: 1.182\n","[8,   600] loss: 1.199\n","[8,   800] loss: 1.243\n","[8,  1000] loss: 1.168\n","[8,  1200] loss: 1.183\n","[8,  1400] loss: 1.200\n","[8,  1600] loss: 1.181\n","[8,  1800] loss: 1.190\n","[8,  2000] loss: 1.178\n","[8,  2200] loss: 1.138\n","[8,  2400] loss: 1.233\n","[8,  2600] loss: 1.229\n","[8,  2800] loss: 1.237\n","[8,  3000] loss: 1.193\n","[8,  3200] loss: 1.198\n","[8,  3400] loss: 1.176\n","[8,  3600] loss: 1.223\n","[8,  3800] loss: 1.304\n","[8,  4000] loss: 1.171\n","[8,  4200] loss: 1.234\n","[8,  4400] loss: 1.182\n","[8,  4600] loss: 1.241\n","[8,  4800] loss: 1.260\n","[8,  5000] loss: 1.265\n","[8,  5200] loss: 1.248\n","[8,  5400] loss: 1.162\n","[8,  5600] loss: 1.184\n","[8,  5800] loss: 1.271\n","[8,  6000] loss: 1.188\n","[8,  6200] loss: 1.193\n","[8,  6400] loss: 1.157\n","[8,  6600] loss: 1.214\n","[8,  6800] loss: 1.242\n","[8,  7000] loss: 1.219\n","[8,  7200] loss: 1.139\n","[8,  7400] loss: 1.178\n","[8,  7600] loss: 1.247\n","[8,  7800] loss: 1.228\n","[8,  8000] loss: 1.215\n","[8,  8200] loss: 1.153\n","[8,  8400] loss: 1.218\n","[8,  8600] loss: 1.271\n","[8,  8800] loss: 1.238\n","[8,  9000] loss: 1.215\n","[8,  9200] loss: 1.206\n","[8,  9400] loss: 1.251\n","[8,  9600] loss: 1.234\n","[8,  9800] loss: 1.195\n","[8, 10000] loss: 1.207\n","[8, 10200] loss: 1.233\n","[8, 10400] loss: 1.286\n","[8, 10600] loss: 1.160\n","[8, 10800] loss: 1.294\n","[8, 11000] loss: 1.158\n","[8, 11200] loss: 1.268\n","[8, 11400] loss: 1.202\n","[8, 11600] loss: 1.269\n","[8, 11800] loss: 1.226\n","[8, 12000] loss: 1.217\n","[8, 12200] loss: 1.174\n","[8, 12400] loss: 1.179\n","[9,   200] loss: 1.137\n","[9,   400] loss: 1.111\n","[9,   600] loss: 1.132\n","[9,   800] loss: 1.137\n","[9,  1000] loss: 1.232\n","[9,  1200] loss: 1.184\n","[9,  1400] loss: 1.194\n","[9,  1600] loss: 1.245\n","[9,  1800] loss: 1.180\n","[9,  2000] loss: 1.201\n","[9,  2200] loss: 1.235\n","[9,  2400] loss: 1.205\n","[9,  2600] loss: 1.125\n","[9,  2800] loss: 1.163\n","[9,  3000] loss: 1.224\n","[9,  3200] loss: 1.246\n","[9,  3400] loss: 1.150\n","[9,  3600] loss: 1.216\n","[9,  3800] loss: 1.179\n","[9,  4000] loss: 1.181\n","[9,  4200] loss: 1.209\n","[9,  4400] loss: 1.194\n","[9,  4600] loss: 1.157\n","[9,  4800] loss: 1.224\n","[9,  5000] loss: 1.249\n","[9,  5200] loss: 1.177\n","[9,  5400] loss: 1.303\n","[9,  5600] loss: 1.269\n","[9,  5800] loss: 1.209\n","[9,  6000] loss: 1.243\n","[9,  6200] loss: 1.177\n","[9,  6400] loss: 1.209\n","[9,  6600] loss: 1.233\n","[9,  6800] loss: 1.156\n","[9,  7000] loss: 1.211\n","[9,  7200] loss: 1.248\n","[9,  7400] loss: 1.124\n","[9,  7600] loss: 1.284\n","[9,  7800] loss: 1.256\n","[9,  8000] loss: 1.190\n","[9,  8200] loss: 1.224\n","[9,  8400] loss: 1.227\n","[9,  8600] loss: 1.253\n","[9,  8800] loss: 1.194\n","[9,  9000] loss: 1.251\n","[9,  9200] loss: 1.176\n","[9,  9400] loss: 1.184\n","[9,  9600] loss: 1.184\n","[9,  9800] loss: 1.267\n","[9, 10000] loss: 1.220\n","[9, 10200] loss: 1.216\n","[9, 10400] loss: 1.163\n","[9, 10600] loss: 1.156\n","[9, 10800] loss: 1.177\n","[9, 11000] loss: 1.151\n","[9, 11200] loss: 1.212\n","[9, 11400] loss: 1.166\n","[9, 11600] loss: 1.230\n","[9, 11800] loss: 1.211\n","[9, 12000] loss: 1.214\n","[9, 12200] loss: 1.196\n","[9, 12400] loss: 1.254\n","[10,   200] loss: 1.161\n","[10,   400] loss: 1.137\n","[10,   600] loss: 1.181\n","[10,   800] loss: 1.183\n","[10,  1000] loss: 1.206\n","[10,  1200] loss: 1.189\n","[10,  1400] loss: 1.288\n","[10,  1600] loss: 1.188\n","[10,  1800] loss: 1.193\n","[10,  2000] loss: 1.210\n","[10,  2200] loss: 1.194\n","[10,  2400] loss: 1.178\n","[10,  2600] loss: 1.235\n","[10,  2800] loss: 1.141\n","[10,  3000] loss: 1.133\n","[10,  3200] loss: 1.268\n","[10,  3400] loss: 1.180\n","[10,  3600] loss: 1.196\n","[10,  3800] loss: 1.183\n","[10,  4000] loss: 1.205\n","[10,  4200] loss: 1.170\n","[10,  4400] loss: 1.210\n","[10,  4600] loss: 1.165\n","[10,  4800] loss: 1.176\n","[10,  5000] loss: 1.203\n","[10,  5200] loss: 1.161\n","[10,  5400] loss: 1.137\n","[10,  5600] loss: 1.123\n","[10,  5800] loss: 1.204\n","[10,  6000] loss: 1.219\n","[10,  6200] loss: 1.213\n","[10,  6400] loss: 1.164\n","[10,  6600] loss: 1.136\n","[10,  6800] loss: 1.172\n","[10,  7000] loss: 1.122\n","[10,  7200] loss: 1.184\n","[10,  7400] loss: 1.137\n","[10,  7600] loss: 1.341\n","[10,  7800] loss: 1.247\n","[10,  8000] loss: 1.181\n","[10,  8200] loss: 1.122\n","[10,  8400] loss: 1.156\n","[10,  8600] loss: 1.160\n","[10,  8800] loss: 1.163\n","[10,  9000] loss: 1.229\n","[10,  9200] loss: 1.172\n","[10,  9400] loss: 1.222\n","[10,  9600] loss: 1.177\n","[10,  9800] loss: 1.138\n","[10, 10000] loss: 1.212\n","[10, 10200] loss: 1.223\n","[10, 10400] loss: 1.120\n","[10, 10600] loss: 1.158\n","[10, 10800] loss: 1.191\n","[10, 11000] loss: 1.089\n","[10, 11200] loss: 1.185\n","[10, 11400] loss: 1.142\n","[10, 11600] loss: 1.245\n","[10, 11800] loss: 1.219\n","[10, 12000] loss: 1.211\n","[10, 12200] loss: 1.276\n","[10, 12400] loss: 1.251\n","\n","\n","Finished Training.\n"]}]},{"cell_type":"code","source":["summary(net3, (3, 32, 32))\n","train(net3, optimizer3, trainloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"id":"3Gyr_4sGMrud","executionInfo":{"status":"error","timestamp":1685366796681,"user_tz":-330,"elapsed":598,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}},"outputId":"19ae7101-e2ed-42bf-cf6f-f17b64b27e97"},"execution_count":68,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-0e85349f7f92>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-65-d82c1b51c213>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m             for hook_id, hook in (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x256 and 144x20)"]}]},{"cell_type":"code","source":["summary(net4, (3, 32, 32))\n","train(net4, optimizer4, trainloader)"],"metadata":{"id":"O99tkbHWQg05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(net5, (3, 32, 32))\n","train(net5, optimizer5, trainloader)"],"metadata":{"id":"CKT7FAN0QhKS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy1 = test(net1, testloader)\n","\n","print(\"Accuracy 1:\", accuracy1)"],"metadata":{"id":"G6shO-HbQhSY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685367054320,"user_tz":-330,"elapsed":7780,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}},"outputId":"dd5e6df5-ddc2-4f37-d729-b22ecc1aae96"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy 1: 10.0\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"tCBcxsm9tgUj"}},{"cell_type":"code","source":["accuracy2 = test(net2, testloader)\n","print(\"Accuracy 2:\", accuracy2)\n"],"metadata":{"id":"dLFLtXLQQhWy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685367141982,"user_tz":-330,"elapsed":7044,"user":{"displayName":"Tanuja Dhope","userId":"02491285439870477769"}},"outputId":"2f2fc838-6182-44c3-bb3b-968b285034ef"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy 2: 10.12\n"]}]},{"cell_type":"code","source":["accuracy1 = test(net1, testloader)\n","accuracy2 = test(net2, testloader)\n","accuracy3 = test(net3, testloader)\n","accuracy4 = test(net4, testloader)\n","accuracy5 = test(net5, testloader)\n","\n","print(\"Accuracy 1:\", accuracy1)\n","print(\"Accuracy 2:\", accuracy2)\n","print(\"Accuracy 3:\", accuracy3)\n","print(\"Accuracy 4:\", accuracy4)\n","print(\"Accuracy 5:\", accuracy5)"],"metadata":{"id":"dk-5ywOxQVIv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xNcsUjX8QhbF"},"execution_count":null,"outputs":[]}]}